{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanjaygandhari/Autism_DL/blob/main/DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81MxS-OEmgL-",
        "outputId": "f431ebd1-4ffc-45e6-9d7e-5b5e13885c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        file_path         label\n",
            "0          /content/consolidated/autistic/465.jpg      autistic\n",
            "1           /content/consolidated/autistic/49.jpg      autistic\n",
            "2          /content/consolidated/autistic/953.jpg      autistic\n",
            "3          /content/consolidated/autistic/994.jpg      autistic\n",
            "4          /content/consolidated/autistic/755.jpg      autistic\n",
            "...                                           ...           ...\n",
            "2921  /content/consolidated/non_autistic/1092.jpg  non_autistic\n",
            "2922   /content/consolidated/non_autistic/803.jpg  non_autistic\n",
            "2923  /content/consolidated/non_autistic/1107.jpg  non_autistic\n",
            "2924  /content/consolidated/non_autistic/1314.jpg  non_autistic\n",
            "2925  /content/consolidated/non_autistic/1028.jpg  non_autistic\n",
            "\n",
            "[2926 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define the path of the uploaded zip file and extraction path\n",
        "zip_path = '/content/consolidated.zip'  # Adjust path if needed\n",
        "extract_path = '/content'  # Path where the zip will be extracted\n",
        "\n",
        "# Step 2: Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)  # Extracts to /content/consolidated\n",
        "\n",
        "# Step 3: Initialize lists to store file paths and labels\n",
        "file_paths = []\n",
        "labels = []\n",
        "\n",
        "# Step 4: Loop through each subfolder (category) within the consolidated folder\n",
        "consolidated_path = os.path.join(extract_path, 'consolidated')\n",
        "for category in ['autistic', 'non_autistic']:\n",
        "    category_path = os.path.join(consolidated_path, category)  # Full path to the subfolder\n",
        "\n",
        "    # Ensure the category path exists to avoid errors\n",
        "    if os.path.isdir(category_path):\n",
        "        # Loop through each file in the category subfolder\n",
        "        for filename in os.listdir(category_path):\n",
        "            # Only add valid image files (e.g., .jpg, .png, .jpeg)\n",
        "            if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "                file_paths.append(os.path.join(category_path, filename))  # Full file path\n",
        "                labels.append(category)  # Use the folder name as the label\n",
        "\n",
        "# Step 5: Create a DataFrame with the collected file paths and labels\n",
        "train_df = pd.DataFrame({\n",
        "    'file_path': file_paths,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "# Step 6: Display the DataFrame with only the file paths and labels\n",
        "print(train_df[['file_path', 'label']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-aKfdGiem_dX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf  # Import TensorFlow\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator( # Use 'tf' instead of 'tensorflow'\n",
        "    rotation_range=15,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input,  # Use 'tf'\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator( # Use 'tf' instead of 'tensorflow'\n",
        "    preprocessing_function=tf.keras.applications.inception_v3.preprocess_input,  # Use 'tf'\n",
        "    rescale=1./255,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6vfx7_5q6Fn",
        "outputId": "c89f2d97-9efb-4cdb-94c4-4be6feac67cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2048 validated image filenames belonging to 2 classes.\n",
            "Found 439 validated image filenames belonging to 2 classes.\n",
            "Found 439 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Assuming 'train_df' was created or loaded earlier\n",
        "# Split into train, validation, and test sets\n",
        "train, temp = train_test_split(train_df, test_size=0.3, random_state=42)  # Split into train and temporary (test + validation)\n",
        "valid, test = train_test_split(temp, test_size=0.5, random_state=42)      # Split temporary into validation and test\n",
        "\n",
        "# Rename the 'label' column to 'label_name' for consistency\n",
        "train = train.rename(columns={'label': 'label_name'})\n",
        "valid = valid.rename(columns={'label': 'label_name'})\n",
        "test = test.rename(columns={'label': 'label_name'})\n",
        "\n",
        "# ImageDataGenerator for augmentation (customize as needed)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,       # Rescale pixel values\n",
        "    rotation_range=20,     # Randomly rotate images\n",
        "    width_shift_range=0.2, # Randomly shift images horizontally\n",
        "    height_shift_range=0.2 # Randomly shift images vertically\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)  # Only rescaling for validation and test sets\n",
        "\n",
        "# Prepare generators\n",
        "train_imgs = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train,\n",
        "    x_col='file_path',\n",
        "    y_col='label_name',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "valid_imgs = test_datagen.flow_from_dataframe(  # Use test_datagen for validation\n",
        "    dataframe=valid,\n",
        "    x_col='file_path',\n",
        "    y_col='label_name',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=20,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "test_imgs = test_datagen.flow_from_dataframe(  # Use test_datagen for the test set\n",
        "    dataframe=test,\n",
        "    x_col='file_path',\n",
        "    y_col='label_name',\n",
        "    target_size=(224, 224),\n",
        "    class_mode='categorical',\n",
        "    batch_size=20,\n",
        "    shuffle=False  # No shuffling for test set to maintain order\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "38OGk-Hur-t4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming train_df was created in the previous step\n",
        "train, test = train_test_split(train_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq1TfA19F69n",
        "outputId": "1ccd0a49-64b5-4a16-9801-de5219aac262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['autistic', 'non_autistic']\n"
          ]
        }
      ],
      "source": [
        "# Get class names from the 'train_imgs' generator\n",
        "class_names = list(train_imgs.class_indices.keys())\n",
        "\n",
        "# Display the class names\n",
        "print(class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZaFZ02Nv4j4i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6D3di3MsWs0",
        "outputId": "b7699fab-6093-4de2-807d-10041ac5c01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "#Part 2 Building the CNN\n",
        "#Initialising the CNN\n",
        "cnn = tf.keras.models.Sequential()\n",
        "\n",
        "#Step 1 Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, padding=\"same\", kernel_size=3, activation='relu', strides=2, input_shape=[64, 64, 3]))\n",
        "\n",
        "#Step 2 Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Adding a second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, padding='same', kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "#Step 3 Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "\n",
        "# Step 4 Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "#Step 5 Output Layer\n",
        "#cnn.add(tf.keras.layers. Dense(units-1, activation 'sigmoid'))\n",
        "##For Binary Classification\n",
        "# Use tf.keras.layers.Dense to define the Dense layer\n",
        "cnn.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='linear'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2BFzGx1sr6G",
        "outputId": "2e2b65af-56b2-47a5-f87f-1d08f1b457e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 2s/step - accuracy: 0.5434 - loss: 0.9464 - val_accuracy: 0.6241 - val_loss: 0.6686\n",
            "Epoch 2/40\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 2s/step - accuracy: 0.6194 - loss: 0.6756 - val_accuracy: 0.6720 - val_loss: 0.6511\n",
            "Epoch 3/40\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 2s/step - accuracy: 0.6378 - loss: 0.6600 - val_accuracy: 0.6538 - val_loss: 0.6397\n",
            "Epoch 4/40\n",
            "\u001b[1m103/103\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 2s/step - accuracy: 0.6556 - loss: 0.6440 - val_accuracy: 0.6606 - val_loss: 0.6281\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Initialize a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an input layer with the specified shape (adjust as per your images)\n",
        "model.add(Input(shape=(224, 224, 3)))  # 224x224 with 3 channels (RGB)\n",
        "\n",
        "# Add convolutional and pooling layers\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add a fully connected layer and output layer\n",
        "# Changed to 2 units and softmax activation for binary classification\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(2, activation='softmax'))  # Modified output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks1= [EarlyStopping(monitor='val_accuracy',\n",
        "min_delta=0,\n",
        "patience=2,\n",
        "mode='auto')]\n",
        "\n",
        "# Fit the model\n",
        "# Change 'valid_gen' to 'valid_imgs'\n",
        "history = model.fit(train_imgs, validation_data=valid_imgs, epochs=40, callbacks=callbacks1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JB47XA6-71P",
        "outputId": "def3848c-d51a-4c7e-dd1d-1b38504a6c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('/content/my_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mn41turCxocT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519ed8d6-0d85-4776-a0e5-4a724c2f8e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss : 0.62325\n",
            "Test Accuracy: 66.29%\n"
          ]
        }
      ],
      "source": [
        "# Testing the model\n",
        "results = model.evaluate(test_imgs, verbose=0)\n",
        "print(\"Test Loss : {:.5f}\".format(results[0]))\n",
        "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))"
      ]
    },
    {
      "source": [
        "!pip install streamlit"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stRnj91yxSfU",
        "outputId": "e8e33963-015c-4d27-90e2-d5432613f32f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.42.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.5)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.25.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.42.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.42.0 watchdog-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "# Load the trained model\n",
        "@st.cache(allow_output_mutation=True)  # Cache the model to avoid reloading on every run\n",
        "def load_model():\n",
        "    model = tf.keras.models.load_model(\"/content/my_model.h5\")  # Provide the path to your saved model\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "# Function to preprocess and make predictions\n",
        "def preprocess_and_predict(image_data, model):\n",
        "    # Convert image to RGB (if needed)\n",
        "    image = Image.open(image_data).convert(\"RGB\")\n",
        "    # Resize the image to (224, 224) as expected by the model\n",
        "    image = image.resize((224, 224))\n",
        "    # Convert the image to a numpy array\n",
        "    image_array = np.asarray(image) / 255.0  # Normalize pixel values to [0, 1]\n",
        "    # Add batch dimension\n",
        "    image_array = np.expand_dims(image_array, axis=0)\n",
        "    # Make prediction\n",
        "    prediction = model.predict(image_array)\n",
        "    return prediction\n",
        "# Streamlit UI\n",
        "st.title(\"Prediction of Autism Spectrum Condition\")\n",
        "st.text(\"Please upload an image file:\")\n",
        "# File uploader\n",
        "uploaded_file = st.file_uploader(\"Choose an image\", type=[\"jpg\", \"png\", \"jpeg\"])\n",
        "if uploaded_file is not None:\n",
        "    # Display the uploaded image\n",
        "    st.image(uploaded_file, caption=\"Uploaded Image\", use_column_width=True)\n",
        "    st.write(\"Classifying...\")\n",
        "\n",
        "    # Preprocess and predict\n",
        "    predictions = preprocess_and_predict(uploaded_file, model)\n",
        "    score = tf.nn.softmax(predictions[0])  # Apply softmax to interpret as probabilities\n",
        "\n",
        "    # Define class names\n",
        "    class_names = ['autistic', 'non_autistic']  # Replace with actual class names\n",
        "    predicted_class = class_names[np.argmax(score)]\n",
        "    confidence = 100 * np.max(score)\n",
        "\n",
        "    # Display prediction\n",
        "    st.success(f\"This image is most likely: **{predicted_class}** with a confidence of {confidence:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSZtrJGxrYiY",
        "outputId": "1070f5b8-2097-4b75-8e21-442cb1d32f08"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZbGp2SuQpmz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e16490-93a8-437a-e55f-d8cf215dc74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok\n",
        "from pyngrok import ngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5qvjnm860SNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ah1bxbRFlWj",
        "outputId": "0bc63d17-11cb-40c1-81b6-13ff7fd7920f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2p273CQLUHmKf5jEkn8cfT4YkAa_7tySqXBr6GcxqoehNs4na"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0v3uzeHv0ab2",
        "outputId": "342cde23-08ca-4964-d8bc-ca162650c78d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "ngrok.set_auth_token(\"2p273CQLUHmKf5jEkn8cfT4YkAa_7tySqXBr6GcxqoehNs4na\")\n",
        "# Run Streamlit app in the background\n",
        "os.system(\"streamlit run app.py &\")\n",
        "\n",
        "# Open a tunnel on port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vkqya40gbH",
        "outputId": "2bdc947b-aee6-4a48-bbd8-28d86e85d281"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at: NgrokTunnel: \"https://4bfb-34-81-208-107.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqEA0hF5b1vRmj8IOGdUu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}